#!/usr/bin/env python3
"""
å¤šé˜¶æ®µæ€è€ƒ API æœåŠ¡å™¨
å®ç°ï¼šè§„åˆ’ -> é€‰æ‹© -> æ€è€ƒ -> è¯„ä¼° -> åé¦ˆå¾ªç¯
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import asyncio
import aiohttp
import json
import time
from datetime import datetime
import sys
import os
import re

app = Flask(__name__)
CORS(app)

# å¼ºåˆ¶æ¸…ç†è¾“å‡ºç¼“å†² - å¤šé‡ä¿éšœ
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)
os.environ['PYTHONUNBUFFERED'] = '1'

# ç¦ç”¨ Flask çš„ç¼“å†²
import logging
logging.basicConfig(level=logging.INFO, format='%(message)s')
app.logger.setLevel(logging.INFO)

# åŠ è½½é…ç½®
def load_config():
    with open('models_config.json', 'r', encoding='utf-8') as f:
        return json.load(f)

CONFIG = load_config()
MODELS = CONFIG['models']

def log(message):
    """ç»ˆç«¯æ—¥å¿— - å¼ºåˆ¶åˆ·æ–°ç¼“å†²"""
    timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
    output = f"[{timestamp}] {message}"
    # å¤šé‡åˆ·æ–°ç¡®ä¿å®æ—¶æ˜¾ç¤º
    print(output, flush=True)
    sys.stdout.flush()
    sys.stderr.flush()
    # Windowsç‰¹æ®Šå¤„ç†ï¼šå¼ºåˆ¶åˆ·æ–°
    if sys.platform == 'win32':
        import msvcrt
        try:
            msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)
            msvcrt.setmode(sys.stdout.fileno(), os.O_TEXT)
        except:
            pass



async def call_model(session, model_config, messages, call_id="", show_full=False, use_cache=False):
    """è°ƒç”¨å•ä¸ªæ¨¡å‹ï¼Œæ”¯æŒpromptç¼“å­˜"""
    
    # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œç»™systemå’Œå‰é¢çš„useræ¶ˆæ¯æ·»åŠ cache_control
    if use_cache and len(messages) > 0:
        # ç»™systemæ¶ˆæ¯æ·»åŠ ç¼“å­˜æ ‡è®°
        if messages[0].get('role') == 'system':
            messages[0]['cache_control'] = {"type": "ephemeral"}
        
        # å¦‚æœæœ‰å¤šæ¡æ¶ˆæ¯ï¼Œç»™å€’æ•°ç¬¬äºŒæ¡useræ¶ˆæ¯ä¹Ÿæ·»åŠ ç¼“å­˜ï¼ˆé€šå¸¸æ˜¯ç”¨æˆ·é—®é¢˜+å†å²æ‘˜è¦ï¼‰
        if len(messages) >= 2:
            for i in range(len(messages) - 1):
                if messages[i].get('role') == 'user':
                    messages[i]['cache_control'] = {"type": "ephemeral"}
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {model_config['api_key']}"
    }
    
    # æ ¹æ®è°ƒç”¨ç±»å‹ä¼˜åŒ–max_tokens
    if "è´¨æ£€" in call_id or "æ‘˜è¦" in call_id:
        max_tokens = 300  # æ‘˜è¦å’Œè´¨æ£€ä¸éœ€è¦å¤ªå¤štoken
    elif "æ­¥éª¤" in call_id:
        max_tokens = 600  # æ€è€ƒæ­¥éª¤é™åˆ¶400å­—ï¼Œçº¦600 tokens
    else:
        max_tokens = CONFIG.get('max_tokens', 2000)
    
    payload = {
        "model": model_config['model'],
        "messages": messages,
        "temperature": CONFIG.get('temperature', 0.7),
        "max_tokens": max_tokens
    }
    
    try:
        log(f"ğŸš€ {call_id} è°ƒç”¨ [{model_config['name']}]" + (" [ç¼“å­˜]" if use_cache else ""))
        async with session.post(model_config['api_url'], json=payload, headers=headers) as response:
            response_text = await response.text()
            if response.status == 200:
                data = json.loads(response_text)
                content = data['choices'][0]['message'].get('content', '')
                
                # æ˜¾ç¤ºç¼“å­˜ä½¿ç”¨æƒ…å†µ
                usage = data.get('usage', {})
                if use_cache and 'cache_read_input_tokens' in usage:
                    cache_hit = usage.get('cache_read_input_tokens', 0)
                    cache_create = usage.get('cache_creation_input_tokens', 0)
                    if cache_hit > 0:
                        log(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: {cache_hit} tokens")
                    if cache_create > 0:
                        log(f"ğŸ“ ç¼“å­˜åˆ›å»º: {cache_create} tokens")
                
                # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦æ˜¾ç¤ºå®Œæ•´å†…å®¹
                if show_full:
                    log(f"âœ… {call_id} å®Œæˆ")
                    log("=" * 80)
                    log("ğŸ“ æ¨¡å‹è¾“å‡ºå†…å®¹:")
                    log(content)
                    log("=" * 80)
                else:
                    log(f"âœ… {call_id} å®Œæˆ: {content[:50]}...")
                return {"success": True, "content": content}
            else:
                log(f"âŒ {call_id} å¤±è´¥: HTTP {response.status}")
                return {"success": False, "error": f"HTTP {response.status}"}
    except Exception as e:
        log(f"âŒ {call_id} å¼‚å¸¸: {str(e)}")
        return {"success": False, "error": str(e)}

# ä¼˜åŒ–çº¿ç¨‹å·²åˆ é™¤ï¼Œèšç„¦ä¸»çº¿ç¨‹

async def generate_middle_summary(session, outputs, start_idx, end_idx, user_message):
    """ç”Ÿæˆä¸­é—´è½®æ¬¡çš„æ‘˜è¦"""
    if start_idx >= end_idx:
        return ""
    
    # åˆå¹¶ä¸­é—´è½®æ¬¡çš„å†…å®¹
    middle_content = []
    total_chars = 0
    for i in range(start_idx, end_idx):
        if i < len(outputs):
            middle_content.append(f"ç¬¬{i+1}è½®ï¼š{outputs[i]}")
            total_chars += len(outputs[i])
    
    if not middle_content:
        return ""
    
    combined_content = "\n\n".join(middle_content)
    log(f"      ğŸ“ åŸå§‹å†…å®¹: {total_chars}å­— â†’ å‡†å¤‡å‹ç¼©...")
    
    # ä½¿ç”¨å°æ¨¡å‹ç”Ÿæˆæ‘˜è¦
    summarizer = MODELS['qwen2_7b']
    
    messages = [
        {"role": "system", "content": """# AIä¸Šä¸‹æ–‡å·¥ç¨‹å¤§å¸ˆ

## æ ¸å¿ƒåŸåˆ™
ä¿¡æ¯å¯†åº¦ > ä¿¡æ¯é•¿åº¦ | ç»“æ„åŒ– > å¹³é“ºç›´å™

## å·¥ä½œæµç¨‹
1. **åˆ†æ** - ç†è§£åŸå§‹å†…å®¹å’Œç›®æ ‡
2. **ç­›é€‰** - æŒ‰å…³è”åº¦(1-5)ã€é‡è¦æ€§(1-5)ã€æ–°é¢–æ€§(1-5)è¯„åˆ†ç­›é€‰
3. **ç²¾ç‚¼** - æ€»ç»“æç‚¼ã€å®ä½“æå–ã€å…³ç³»æ˜ å°„
4. **ç»“æ„åŒ–** - æŒ‰æŒ‡å®šæ ¼å¼ç»„ç»‡è¾“å‡º

## è¾“å‡ºæ ¼å¼
```markdown
## æ ¸å¿ƒæ‘˜è¦
[2-3å¥æ ¸å¿ƒç»“è®º]

## å…³é”®æ¨ç†é“¾
- æ­¥éª¤1 â†’ æ­¥éª¤2 â†’ ç»“è®º

## é‡è¦å‘ç°
- [åˆ†ç±»]: [å…·ä½“ä¿¡æ¯]

## è½¬æŠ˜ç‚¹
- [å…³é”®çªç ´æˆ–æ–¹å‘è½¬å˜]
```"""},
        {"role": "user", "content": f"""ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
{user_message}

ã€ä¸­é—´è½®æ¬¡å†…å®¹ã€‘ï¼š
{combined_content}

ã€ä»»åŠ¡ã€‘ï¼š
å°†ä¸Šè¿°ä¸­é—´è½®æ¬¡çš„æ€è€ƒå†…å®¹å‹ç¼©æˆé«˜å¯†åº¦æ‘˜è¦ã€‚
- æå–æ ¸å¿ƒç»“è®ºå’Œå…³é”®æ¨ç†é“¾
- ä¿ç•™é‡è¦å‘ç°å’Œè½¬æŠ˜ç‚¹
- å»é™¤é‡å¤ã€å†—ä½™ã€è¿‡æ¸¡å¥
- ä¿¡æ¯å¯†åº¦ä¼˜å…ˆï¼Œé•¿åº¦æ¬¡è¦

è¾“å‡ºæ‘˜è¦ï¼š"""}
    ]
    
    result = await call_model(session, summarizer, messages, "ä¸­é—´æ‘˜è¦", show_full=False)
    
    if result.get('success'):
        summary = result['content'].strip()
        compression_rate = (1 - len(summary) / total_chars) * 100 if total_chars > 0 else 0
        log(f"      âœ… æ‘˜è¦å®Œæˆ: {len(summary)}å­— (å‹ç¼©ç‡: {compression_rate:.1f}%)")
        log(f"      ğŸ“„ æ‘˜è¦å†…å®¹:\n{summary}")
        return summary
    else:
        # å¤±è´¥æ—¶è¿”å›ç®€åŒ–ç‰ˆ
        log(f"      âš ï¸ æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆ")
        return f"ã€ä¸­é—´è½®æ¬¡æ‘˜è¦ã€‘ç¬¬{start_idx+1}-{end_idx}è½®çš„å…³é”®å†…å®¹"

async def get_recent_context(session, all_outputs, step_num, user_message):
    """æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†ï¼šåŠ¨æ€æ‘˜è¦+å®Œæ•´ä¿ç•™"""
    log(f"\nğŸ§  æ„å»ºæ­¥éª¤{step_num}çš„æ™ºèƒ½ä¸Šä¸‹æ–‡...")
    
    if step_num == 1:
        # ç¬¬1è½®ï¼šæ— ä¸Šä¸‹æ–‡
        log("  â†’ ç¬¬1è½®ï¼šæ— ä¸Šä¸‹æ–‡")
        return ""
    elif step_num == 2:
        # ç¬¬2è½®ï¼šç¬¬1è½®å®Œæ•´
        log("  â†’ ç¬¬2è½®ï¼šç¬¬1è½®å®Œæ•´")
        if all_outputs:
            return f"ã€ç¬¬1è½®æ€è€ƒã€‘\n{all_outputs[0]}"
        return ""
    elif step_num == 3:
        # ç¬¬3è½®ï¼šç¬¬1è½®+ç¬¬2è½®å®Œæ•´
        log("  â†’ ç¬¬3è½®ï¼šç¬¬1-2è½®å®Œæ•´")
        context_parts = []
        if len(all_outputs) >= 1:
            context_parts.append(f"ã€ç¬¬1è½®æ€è€ƒã€‘\n{all_outputs[0]}")
        if len(all_outputs) >= 2:
            context_parts.append(f"ã€ç¬¬2è½®æ€è€ƒã€‘\n{all_outputs[1]}")
        return "\n\n".join(context_parts)
    else:
        # ç¬¬4è½®+ï¼šç¬¬1è½®å®Œæ•´ + ä¸­é—´æ‘˜è¦ + æœ€è¿‘2è½®å®Œæ•´
        log(f"  â†’ ç¬¬{step_num}è½®ï¼šæ™ºèƒ½ä¸Šä¸‹æ–‡ç»„è£…")
        context_parts = []
        
        # 1. ç¬¬1è½®å®Œæ•´ï¼ˆæ ¸å¿ƒï¼Œæ°¸ä¹…ä¿ç•™ï¼‰
        if len(all_outputs) >= 1:
            context_parts.append(f"ã€ç¬¬1è½®æ€è€ƒã€‘\n{all_outputs[0]}")
            log(f"    âœ… ç¬¬1è½®å®Œæ•´ä¿ç•™ ({len(all_outputs[0])}å­—)")
        
        # 2. ä¸­é—´è½®æ¬¡æ‘˜è¦ï¼ˆç¬¬2è½®åˆ°å€’æ•°ç¬¬3è½®ï¼‰
        if step_num > 4:  # åªæœ‰è¶…è¿‡4è½®æ‰æœ‰ä¸­é—´è½®æ¬¡
            middle_start = 1  # ç¬¬2è½®å¼€å§‹ï¼ˆç´¢å¼•1ï¼‰
            middle_end = step_num - 3  # åˆ°å€’æ•°ç¬¬3è½®ï¼ˆä¸åŒ…å«æœ€è¿‘2è½®ï¼‰
            if middle_end > middle_start:
                log(f"    ğŸ”„ ç”Ÿæˆç¬¬{middle_start+1}-{middle_end}è½®æ‘˜è¦...")
                middle_summary = await generate_middle_summary(session, all_outputs, middle_start, middle_end, user_message)
                if middle_summary:
                    context_parts.append(middle_summary)
                    log(f"    âœ… ä¸­é—´æ‘˜è¦ç”Ÿæˆå®Œæˆ ({len(middle_summary)}å­—)")
        
        # 3. æœ€è¿‘2è½®å®Œæ•´ï¼ˆä¿æŒè¿è´¯æ€§ï¼‰
        recent_start = max(0, step_num - 3)  # å€’æ•°ç¬¬2è½®
        for i in range(recent_start, step_num - 1):  # ä¸åŒ…å«å½“å‰è½®
            if i < len(all_outputs):
                context_parts.append(f"ã€ç¬¬{i+1}è½®æ€è€ƒã€‘\n{all_outputs[i]}")
                log(f"    âœ… ç¬¬{i+1}è½®å®Œæ•´ä¿ç•™ ({len(all_outputs[i])}å­—)")
        
        final_context = "\n\n".join(context_parts)
        log(f"  ğŸ“Š ä¸Šä¸‹æ–‡ç»„è£…å®Œæˆï¼Œæ€»é•¿åº¦: {len(final_context)} å­—ç¬¦")
        return final_context

async def validate_step_async(session, step_content, user_message, step_num):
    """å¼‚æ­¥éªŒè¯çº¿ç¨‹ï¼šåä¾‹ç”Ÿæˆ + æŠ•ç¥¨å¯¹æŠ—æœºåˆ¶"""
    log(f"\n" + "ğŸ”" * 40)
    log(f"[éªŒè¯çº¿ç¨‹] æ­¥éª¤{step_num} - å¯åŠ¨åä¾‹å¯¹æŠ—éªŒè¯")
    log("ğŸ”" * 40)
    log(f"ğŸ“ [éªŒè¯çº¿ç¨‹] ä¸»çº¿ç¨‹è¾“å‡ºé•¿åº¦: {len(step_content)} å­—ç¬¦")
    
    # ========== é˜¶æ®µ1ï¼šç”Ÿæˆåä¾‹ï¼ˆ3ä¸ªQwen2.5-7Bå¹¶è¡Œï¼‰ ==========
    log(f"\nğŸ­ [é˜¶æ®µ1] å¯åŠ¨3ä¸ªåä¾‹ç”Ÿæˆå™¨ï¼ˆQwen2.5-7Bï¼‰...")
    
    counterexample_generators = [
        MODELS['qwen2_5_7b'],
        MODELS['qwen2_5_7b'],
        MODELS['qwen2_5_7b']
    ]
    
    # åä¾‹ç”Ÿæˆpromptï¼ˆåªç»™å½“å‰æ­¥éª¤+ç”¨æˆ·æŒ‡ä»¤ï¼‰
    counterexample_prompt = f"""ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
{user_message}

ã€ä¸»çº¿ç¨‹çš„æ€è€ƒã€‘ï¼š
{step_content}

ã€ä½ çš„ä»»åŠ¡ã€‘ï¼š
ä½ æ˜¯åä¾‹ç”Ÿæˆä¸“å®¶ï¼Œä¸“é—¨"å¯¹ç€å¹²"ã€‚è¯·é’ˆå¯¹ä¸»çº¿ç¨‹çš„æ€è€ƒï¼Œç”Ÿæˆä¸€ä¸ªæœ‰åŠ›çš„åä¾‹æˆ–åé©³è§‚ç‚¹ã€‚

è¦æ±‚ï¼š
1. æ‰¾å‡ºä¸»çº¿ç¨‹æ€è€ƒçš„æ¼æ´ã€ç›²ç‚¹ã€é”™è¯¯
2. æå‡ºç›¸åçš„è§‚ç‚¹æˆ–åä¾‹
3. é€»è¾‘ä¸¥å¯†ï¼Œæœ‰ç†æœ‰æ®
4. ä¸è¦é‡å¤ä¸»çº¿ç¨‹çš„å†…å®¹

è¾“å‡ºæ ¼å¼ï¼š
ã€åä¾‹ã€‘
[ä½ çš„åé©³è§‚ç‚¹æˆ–åä¾‹]"""
    
    # å¹¶è¡Œç”Ÿæˆ3ä¸ªåä¾‹
    counterexample_tasks = []
    for i, generator in enumerate(counterexample_generators):
        messages = [
            {"role": "system", "content": "ä½ æ˜¯åä¾‹ç”Ÿæˆä¸“å®¶ï¼Œä¸“é—¨æ‰¾æ¼æ´ã€æåé©³ã€‚"},
            {"role": "user", "content": counterexample_prompt}
        ]
        counterexample_tasks.append(
            call_model(session, generator, messages, f"åä¾‹ç”Ÿæˆå™¨{i+1}", show_full=False)
        )
    
    counterexample_results = await asyncio.gather(*counterexample_tasks)
    
    # æå–åä¾‹å†…å®¹
    counterexamples = []
    for i, result in enumerate(counterexample_results):
        if result.get('success'):
            content = result['content'].strip()
            counterexamples.append(content)
            log(f"  âœ… åä¾‹{i+1}ç”Ÿæˆå®Œæˆ ({len(content)}å­—)")
            log(f"     ğŸ“„ å†…å®¹é¢„è§ˆ: {content[:100]}...")
        else:
            log(f"  âŒ åä¾‹{i+1}ç”Ÿæˆå¤±è´¥")
    
    if not counterexamples:
        log(f"âš ï¸  [éªŒè¯çº¿ç¨‹] åä¾‹ç”Ÿæˆå…¨éƒ¨å¤±è´¥ï¼Œè·³è¿‡éªŒè¯")
        return True, None
    
    # ========== é˜¶æ®µ2ï¼šæŠ•ç¥¨å¯¹æŠ—ï¼ˆ3ä¸ªGLM-4-9Bå¹¶è¡Œï¼‰ ==========
    log(f"\nğŸ—³ï¸  [é˜¶æ®µ2] å¯åŠ¨3ä¸ªæŠ•ç¥¨å™¨ï¼ˆGLM-4-9Bï¼‰...")
    
    voters = [
        MODELS['glm4_9b'],
        MODELS['glm4_9b'],
        MODELS['glm4_9b']
    ]
    
    # æŠ•ç¥¨promptï¼ˆåŒ…å«ä¸»çº¿ç¨‹è¾“å‡º+3ä¸ªåä¾‹+ç”¨æˆ·æŒ‡ä»¤ï¼‰
    counterexamples_text = "\n\n".join([
        f"ã€åä¾‹{i+1}ã€‘\n{ce}" for i, ce in enumerate(counterexamples)
    ])
    
    voting_prompt = f"""ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
{user_message}

ã€ä¸»çº¿ç¨‹çš„æ€è€ƒã€‘ï¼š
{step_content}

ã€åä¾‹è§‚ç‚¹ã€‘ï¼š
{counterexamples_text}

ã€æŠ•ç¥¨ä»»åŠ¡ã€‘ï¼š
ä½ æ˜¯å…¬æ­£çš„è¯„å®¡ä¸“å®¶ã€‚è¯·ç»¼åˆè¯„ä¼°ä¸»çº¿ç¨‹çš„æ€è€ƒå’Œåä¾‹è§‚ç‚¹ï¼Œåˆ¤æ–­è°æ›´æœ‰é“ç†ã€‚

è¯„åˆ†æ ‡å‡†ï¼š
- ä¸»çº¿ç¨‹ï¼šé€»è¾‘æ­£ç¡®ã€ç´§æ‰£é—®é¢˜ã€æœ‰ä»·å€¼
- åä¾‹ï¼šæ‰¾åˆ°äº†çœŸå®çš„æ¼æ´æˆ–ç›²ç‚¹

æŠ•ç¥¨è§„åˆ™ï¼š
âœ… æŠ•ç»™ä¸»çº¿ç¨‹ï¼šä¸»çº¿ç¨‹æ€è€ƒæ­£ç¡®ï¼Œåä¾‹ç«™ä¸ä½è„š
âŒ æŠ•ç»™åä¾‹ï¼šåä¾‹æ‰¾åˆ°äº†çœŸå®é—®é¢˜ï¼Œä¸»çº¿ç¨‹éœ€è¦æ”¹è¿›

è¾“å‡ºæ ¼å¼ï¼š
æŠ•ç¥¨ï¼š[ä¸»çº¿ç¨‹/åä¾‹]
ç†ç”±ï¼š[ä¸€å¥è¯è¯´æ˜]"""
    
    # å¹¶è¡ŒæŠ•ç¥¨
    voting_tasks = []
    for i, voter in enumerate(voters):
        messages = [
            {"role": "system", "content": "ä½ æ˜¯å…¬æ­£çš„è¯„å®¡ä¸“å®¶ï¼Œå®¢è§‚è¯„åˆ¤ã€‚"},
            {"role": "user", "content": voting_prompt}
        ]
        voting_tasks.append(
            call_model(session, voter, messages, f"æŠ•ç¥¨å™¨{i+1}", show_full=False)
        )
    
    voting_results = await asyncio.gather(*voting_tasks)
    
    # ç»Ÿè®¡æŠ•ç¥¨
    main_votes = 0
    counter_votes = 0
    vote_reasons = []
    
    log(f"\nğŸ“Š [æŠ•ç¥¨ç»Ÿè®¡]ï¼š")
    for i, result in enumerate(voting_results):
        if result.get('success'):
            content = result['content']
            if "ä¸»çº¿ç¨‹" in content and "åä¾‹" not in content.split("æŠ•ç¥¨ï¼š")[1].split("\n")[0]:
                main_votes += 1
                log(f"  âœ… æŠ•ç¥¨å™¨{i+1}ï¼šæŠ•ç»™ä¸»çº¿ç¨‹")
            else:
                counter_votes += 1
                log(f"  âŒ æŠ•ç¥¨å™¨{i+1}ï¼šæŠ•ç»™åä¾‹")
                # æå–ç†ç”±
                if "ç†ç”±ï¼š" in content:
                    reason = content.split("ç†ç”±ï¼š")[1].strip()
                    vote_reasons.append(f"æŠ•ç¥¨å™¨{i+1}: {reason}")
                    log(f"     ğŸ’¬ ç†ç”±: {reason}")
    
    log(f"\nğŸ“Š [æœ€ç»ˆç»“æœ] ä¸»çº¿ç¨‹: {main_votes}ç¥¨ | åä¾‹: {counter_votes}ç¥¨")
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰“æ–­ï¼ˆ2ç¥¨æˆ–3ç¥¨æŠ•ä¸»çº¿ç¨‹å°±é€šè¿‡ï¼‰
    if main_votes >= 2:
        log(f"âœ… [éªŒè¯çº¿ç¨‹] ä¸»çº¿ç¨‹é€šè¿‡ï¼ˆ{main_votes}/3ç¥¨ï¼‰ï¼Œç»§ç»­")
        log("ğŸ”" * 40 + "\n")
        return True, None
    else:
        log(f"âš ï¸  [éªŒè¯çº¿ç¨‹] ä¸»çº¿ç¨‹å¤±è´¥ï¼ˆä»…{main_votes}/3ç¥¨ï¼‰ï¼Œæ‰“æ–­")
        
        # æ•´åˆåä¾‹ä½œä¸ºåé¦ˆ
        feedback_parts = ["ã€åä¾‹è§‚ç‚¹ã€‘"]
        for i, ce in enumerate(counterexamples):
            feedback_parts.append(f"\nåä¾‹{i+1}ï¼š{ce}")
        
        feedback = "\n".join(feedback_parts)
        log("ğŸ”" * 40 + "\n")
        
        return False, feedback

async def think_step(session, step_num, recent_context, user_message):
    """æ€è€ƒä¸€ä¸ªæ­¥éª¤ï¼ˆ400å­—é™åˆ¶ï¼‰- ç®€åŒ–ç‰ˆ"""
    log("\n" + "=" * 80)
    log(f"ğŸ’­ æ­¥éª¤ {step_num}")
    log("=" * 80)
    
    thinker = MODELS['planner']
    
    # ç»Ÿä¸€çš„é«˜å¯†åº¦æ€è€ƒprompt
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ·±åº¦æ€è€ƒä¸“å®¶ã€‚

æ ¸å¿ƒä»»åŠ¡ï¼šå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦åç¦»ä¸»é¢˜ã€‚

é‡è¦è§„åˆ™ï¼š
1. æ¯ä¸ªæ­¥éª¤é™åˆ¶400å­—
2. ç´§æ‰£ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦å‘æ•£
3. è¿›è¡Œæ·±åº¦å¤šè½®æ€è€ƒï¼Œå……åˆ†åˆ†æåæ‰èƒ½å®Œæˆ
4. åªæœ‰åœ¨å®Œå…¨è§£å†³é—®é¢˜åæ‰èƒ½æ ‡æ³¨ã€å®Œæˆã€‘
5. æ€è€ƒè½®æ¬¡æ— ä¸Šé™ï¼Œéœ€è¦å¤šå°‘è½®å°±æ€è€ƒå¤šå°‘è½®

æ€è€ƒå¯†åº¦è¦æ±‚ï¼ˆæ¯å¥è¯å¿…é¡»æœ‰ä»·å€¼ï¼‰ï¼š
âœ… æ¯å¥è¯éƒ½å¿…é¡»æ¨è¿›æ€è€ƒ
âœ… æ¯å¥è¯éƒ½æ˜¯æ–°ä¿¡æ¯æˆ–æ–°æ¨ç†
âœ… ç›´æ¥ç»™å‡ºåˆ†æï¼Œä¸è¦é“ºå«

ä¸¥ç¦è¾“å‡ºï¼ˆ0å®¹å¿ï¼‰ï¼š
âŒ é‡å¤å·²è¯´è¿‡çš„å†…å®¹
âŒ ç©ºæ´çš„è¿‡æ¸¡å¥ï¼ˆ"æ¥ä¸‹æ¥"ã€"ç„¶å"ã€"ç»¼ä¸Šæ‰€è¿°"ï¼‰
âŒ æ— æ„ä¹‰çš„æ€»ç»“å’Œå¤è¿°
âŒ ä¸é—®é¢˜æ— å…³çš„å»¶ä¼¸
âŒ å•°å—¦çš„è§£é‡Šå’ŒåºŸè¯

æ ¼å¼ï¼š
- å®Œæˆæ—¶ï¼š[æ€è€ƒå†…å®¹]ã€å®Œæˆã€‘
- ç»§ç»­æ—¶ï¼š[æ€è€ƒå†…å®¹]ã€ç»§ç»­ã€‘"""

    # æ„å»ºæ¶ˆæ¯ï¼šæ¯è½®éƒ½é‡å¤åŸå§‹é—®é¢˜ï¼Œé˜²æ­¢åç¦»
    user_content = f"ã€ç”¨æˆ·çš„åŸå§‹é—®é¢˜ã€‘ï¼š{user_message}"
    if recent_context:
        user_content += f"\n\nã€ä¹‹å‰çš„æ€è€ƒæ‘˜è¦ã€‘ï¼š\n{recent_context}"
    
    # å¼ºè°ƒå½“å‰è½®æ¬¡
    user_content += f"\n\nã€å½“å‰ä»»åŠ¡ã€‘ï¼šè¿™æ˜¯ç¬¬{step_num}è½®æ€è€ƒã€‚ç»§ç»­æ·±å…¥åˆ†æï¼Œåªæœ‰å®Œå…¨è§£å†³é—®é¢˜åæ‰æ ‡æ³¨ã€å®Œæˆã€‘ã€‚"
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content}
    ]
    
    if recent_context:
        log("ğŸ“¦ æ³¨å…¥çš„ä¸Šä¸‹æ–‡:")
        log(recent_context)
        log("")
    
    log(f"ğŸš€ å¼€å§‹æ€è€ƒ...")
    log(f"ğŸ“‹ ä½¿ç”¨æ¨¡å‹: {thinker['name']}")
    result = await call_model(session, thinker, messages, f"æ­¥éª¤{step_num}", show_full=True, use_cache=True)
    
    if not result.get('success'):
        log(f"âŒ æ€è€ƒå¤±è´¥")
        return None, False, False
    
    content = result['content']
    
    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
    is_complete = "ã€å®Œæˆã€‘" in content
    need_continue = "ã€ç»§ç»­ã€‘" in content
    
    log("\n" + "=" * 80)
    log(f"âœ… æ­¥éª¤{step_num}å®Œæˆ" + (" [æœ€ç»ˆç­”æ¡ˆ]" if is_complete else " [éœ€è¦ç»§ç»­]"))
    log("=" * 80)
    log("ğŸ“ è¾“å‡ºå†…å®¹:")
    log(content)
    log("=" * 80 + "\n")
    
    return content, is_complete, need_continue

async def single_thinking_thread(user_message, thread_id, session):
    """å•ä¸ªæ€è€ƒçº¿ç¨‹"""
    log(f"\nğŸ§µ [çº¿ç¨‹{thread_id}] å¯åŠ¨æ€è€ƒæµç¨‹")
    
    # ä¸‰çº¿ç¨‹æ¶æ„
    all_outputs = []  # ä¿å­˜æ‰€æœ‰æ­¥éª¤çš„åŸå§‹è¾“å‡º
    final_answers = []  # ä¿å­˜ä¼˜åŒ–åçš„ç­”æ¡ˆï¼ˆè¿”å›ç»™å‰ç«¯ï¼‰
    
    step_num = 0
    is_complete = False
    
    # åå°éªŒè¯çº¿ç¨‹ç®¡ç†
    pending_validations = {}  # {step_num: validation_task}
    validation_results = {}  # {step_num: (passed, feedback)}
    
    while not is_complete:
        step_num += 1
        
        log(f"\nğŸ§µ [çº¿ç¨‹{thread_id}] æ­¥éª¤ {step_num}")
        
        # è·å–æ™ºèƒ½ä¸Šä¸‹æ–‡ï¼ˆåŠ¨æ€æ‘˜è¦+å®Œæ•´ä¿ç•™ï¼‰
        recent_context = await get_recent_context(session, all_outputs, step_num, user_message)
        
        # ä¸»çº¿ç¨‹ï¼šæ­£å¸¸æ€è€ƒ
        answer, model_complete, need_continue = await think_step(session, step_num, recent_context, user_message)
        
        if not answer:
            log(f"âš ï¸  [çº¿ç¨‹{thread_id}] æ­¥éª¤{step_num}æ€è€ƒå¤±è´¥")
            continue
        
        # æ¸…ç†ç­”æ¡ˆ
        clean_answer = answer
        if "ã€ç»§ç»­ã€‘" in clean_answer:
            clean_answer = clean_answer.split("ã€ç»§ç»­ã€‘")[0].strip()
        if "ã€å®Œæˆã€‘" in clean_answer:
            clean_answer = clean_answer.replace("ã€å®Œæˆã€‘", "").strip()
        
        # ä¿å­˜è¾“å‡ºï¼ˆä¸ç­‰å¾…éªŒè¯ï¼‰
        all_outputs.append(clean_answer)
        final_answers.append(clean_answer)
        log(f"ğŸ’¾ [çº¿ç¨‹{thread_id}] æ­¥éª¤{step_num}è¾“å‡ºå·²ä¿å­˜")
        
        # å¯åŠ¨åå°éªŒè¯çº¿ç¨‹ï¼ˆä¸ç­‰å¾…ï¼‰
        current_step_for_validation = step_num
        current_answer_for_validation = clean_answer
        
        async def validate_in_background():
            """åå°éªŒè¯çº¿ç¨‹ - å‘ç°é—®é¢˜æ—¶ç›´æ¥æ‰“æ–­ä¸»çº¿ç¨‹"""
            nonlocal is_complete, step_num, all_outputs, final_answers
            
            passed, feedback = await validate_step_async(
                session, current_answer_for_validation, user_message, current_step_for_validation
            )
            
            if not passed:
                # éªŒè¯å¤±è´¥ï¼Œæ‰“æ–­ä¸»çº¿ç¨‹
                log(f"\nğŸš¨ [çº¿ç¨‹{thread_id}] æ­¥éª¤{current_step_for_validation}éªŒè¯å¤±è´¥ï¼")
                if current_step_for_validation < len(all_outputs):
                    all_outputs[current_step_for_validation - 1] = None  # æ ‡è®°ä¸ºéœ€è¦é‡åš
                    validation_results[current_step_for_validation] = (False, feedback)
        
        pending_validations[step_num] = asyncio.create_task(validate_in_background())
        
        # ä¸»çº¿ç¨‹ç»§ç»­ï¼Œä¸ç­‰å¾…éªŒè¯ç»“æœ
        
        # åªæ£€æŸ¥ä¸»æ¨¡å‹åˆ¤æ–­
        if model_complete:
            log(f"âœ… [çº¿ç¨‹{thread_id}] ä¸»æ¨¡å‹åˆ¤æ–­ï¼šå·²å®Œæˆ")
            is_complete = True
        
        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
        if is_complete:
            break
    
    # ç­‰å¾…æ‰€æœ‰éªŒè¯å®Œæˆ
    if pending_validations:
        await asyncio.gather(*pending_validations.values(), return_exceptions=True)
    
    log(f"\nâœ… [çº¿ç¨‹{thread_id}] æ€è€ƒæµç¨‹å®Œæˆï¼Œå…±{len(final_answers)}æ­¥")
    
    return {
        'thread_id': thread_id,
        'steps': final_answers,
        'total_steps': len(final_answers)
    }

async def multi_stage_thinking(user_message):
    """å¤šé˜¶æ®µæ€è€ƒä¸»æµç¨‹ - ä¸‰é‡å¹¶è¡Œæ¶æ„"""
    log(f"ğŸ¯ å¼€å§‹ä¸‰é‡å¹¶è¡Œæ€è€ƒæµç¨‹")
    log(f"ğŸ§µ å¯åŠ¨3ä¸ªç‹¬ç«‹æ€è€ƒçº¿ç¨‹...")
    log("")
    
    start_time = time.time()
    
    connector = aiohttp.TCPConnector(limit=150, limit_per_host=150)  # å¢åŠ è¿æ¥æ•°
    timeout = aiohttp.ClientTimeout(total=600)  # å¢åŠ è¶…æ—¶æ—¶é—´
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        # å¹¶è¡Œå¯åŠ¨3ä¸ªç‹¬ç«‹æ€è€ƒçº¿ç¨‹
        thread_tasks = [
            asyncio.create_task(single_thinking_thread(user_message, 1, session)),
            asyncio.create_task(single_thinking_thread(user_message, 2, session)),
            asyncio.create_task(single_thinking_thread(user_message, 3, session))
        ]
        
        log(f"â³ ç­‰å¾…3ä¸ªçº¿ç¨‹å®Œæˆ...")
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        thread_results = await asyncio.gather(*thread_tasks, return_exceptions=True)
        
        log("\n" + "ğŸ‰" * 40)
        log(f"ä¸‰é‡å¹¶è¡Œæ€è€ƒå®Œæˆï¼")
        log("ğŸ‰" * 40 + "\n")
        
        # åˆå¹¶æ‰€æœ‰çº¿ç¨‹çš„ç»“æœ
        all_thread_outputs = []
        for result in thread_results:
            if isinstance(result, dict):
                thread_id = result['thread_id']
                steps = result['steps']
                total = result['total_steps']
                log(f"âœ… çº¿ç¨‹{thread_id}: {total}æ­¥")
                all_thread_outputs.append({
                    'thread_id': thread_id,
                    'steps': steps
                })
        
        log("\nğŸ“¦ å‡†å¤‡å‰ç«¯è¿”å›å†…å®¹...")
        
        # æ ¼å¼åŒ–è¾“å‡ºï¼šæ¯ä¸ªçº¿ç¨‹çš„å†…å®¹
        formatted_answers = []
        
        for thread_data in all_thread_outputs:
            thread_id = thread_data['thread_id']
            steps = thread_data['steps']
            
            # æ·»åŠ çº¿ç¨‹æ ‡é¢˜
            formatted_answers.append(f"\n{'='*60}")
            formatted_answers.append(f"ã€æ€è€ƒçº¿ç¨‹ {thread_id}ã€‘")
            formatted_answers.append(f"{'='*60}\n")
            
            # æ·»åŠ æ¯ä¸€æ­¥
            step_labels = ["ç¬¬ä¸€æ­¥", "ç¬¬äºŒæ­¥", "ç¬¬ä¸‰æ­¥", "ç¬¬å››æ­¥", "ç¬¬äº”æ­¥", "ç¬¬å…­æ­¥", "ç¬¬ä¸ƒæ­¥", "ç¬¬å…«æ­¥", "ç¬¬ä¹æ­¥", "ç¬¬åæ­¥"]
            for i, answer in enumerate(steps):
                clean = answer.replace("[æ€è€ƒå†…å®¹]", "").strip()
                if i < len(step_labels):
                    formatted = f"ã€{step_labels[i]}ã€‘\n{clean}"
                else:
                    formatted = f"ã€ç¬¬{i+1}æ­¥ã€‘\n{clean}"
                formatted_answers.append(formatted)
        
        # æ”¶é›†3ä¸ªçº¿ç¨‹çš„åŸå§‹å†…å®¹
        thread1_content = "\n\n".join([step for thread in all_thread_outputs if thread['thread_id'] == 1 for step in thread['steps']])
        thread2_content = "\n\n".join([step for thread in all_thread_outputs if thread['thread_id'] == 2 for step in thread['steps']])
        thread3_content = "\n\n".join([step for thread in all_thread_outputs if thread['thread_id'] == 3 for step in thread['steps']])
        
        log(f"\nğŸ”„ å¼€å§‹æ™ºèƒ½èåˆ3ä¸ªçº¿ç¨‹çš„å†…å®¹...")
        log(f"ğŸ“Š çº¿ç¨‹1: {len(thread1_content)}å­—")
        log(f"ğŸ“Š çº¿ç¨‹2: {len(thread2_content)}å­—")
        log(f"ğŸ“Š çº¿ç¨‹3: {len(thread3_content)}å­—")
        
        # ä½¿ç”¨KAT-Coderèåˆ3ä¸ªçº¿ç¨‹çš„å†…å®¹
        fusion_model = MODELS['step_selector']  # KAT-Coder-Exp-72B-1010
        
        fusion_prompt = f"""ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
{user_message}

ã€çº¿ç¨‹1çš„æ€è€ƒå†…å®¹ã€‘ï¼š
{thread1_content}

ã€çº¿ç¨‹2çš„æ€è€ƒå†…å®¹ã€‘ï¼š
{thread2_content}

ã€çº¿ç¨‹3çš„æ€è€ƒå†…å®¹ã€‘ï¼š
{thread3_content}

ã€èåˆä»»åŠ¡ã€‘ï¼š
ä½ ç°åœ¨éœ€è¦æ™ºèƒ½èåˆè¿™3ä¸ªç‹¬ç«‹æ€è€ƒçº¿ç¨‹çš„å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€è¿è´¯ã€é«˜è´¨é‡çš„æœ€ç»ˆç­”æ¡ˆã€‚

èåˆè¦æ±‚ï¼š
1. æå–3ä¸ªçº¿ç¨‹çš„å…±åŒç»“è®ºå’Œæ ¸å¿ƒè§‚ç‚¹
2. æ•´åˆä¸åŒçº¿ç¨‹çš„ç‹¬ç‰¹è§è§£å’Œè¡¥å……ä¿¡æ¯
3. è§£å†³çº¿ç¨‹ä¹‹é—´çš„çŸ›ç›¾æˆ–åˆ†æ­§ï¼ˆå¦‚æœæœ‰ï¼‰
4. ç»„ç»‡æˆé€»è¾‘æ¸…æ™°ã€ç»“æ„å®Œæ•´çš„ç­”æ¡ˆ
5. ä¿ç•™å…³é”®çš„æ¨ç†è¿‡ç¨‹å’Œé‡è¦ç»†èŠ‚
6. å»é™¤é‡å¤å’Œå†—ä½™å†…å®¹

è¾“å‡ºæ ¼å¼ï¼š
ç›´æ¥è¾“å‡ºèåˆåçš„å®Œæ•´ç­”æ¡ˆï¼Œä¸è¦æ·»åŠ "èåˆç»“æœ"ç­‰æ ‡é¢˜ã€‚"""

        fusion_messages = [
            {"role": "system", "content": "ä½ æ˜¯å†…å®¹èåˆä¸“å®¶ï¼Œæ“…é•¿æ•´åˆå¤šä¸ªæ€è€ƒçº¿ç¨‹çš„å†…å®¹ã€‚"},
            {"role": "user", "content": fusion_prompt}
        ]
        
        log(f"ğŸ¤– è°ƒç”¨KAT-Coderè¿›è¡Œæ™ºèƒ½èåˆ...")
        fusion_result = await call_model(session, fusion_model, fusion_messages, "å†…å®¹èåˆ", show_full=False)
        
        if fusion_result.get('success'):
            final_output = fusion_result['content'].strip()
            log(f"âœ… èåˆå®Œæˆï¼æœ€ç»ˆå†…å®¹: {len(final_output)}å­—")
        else:
            log(f"âš ï¸  èåˆå¤±è´¥ï¼Œè¿”å›åŸå§‹æ‹¼æ¥å†…å®¹")
            final_output = "\n\n".join(formatted_answers)
        
        elapsed = time.time() - start_time
        log(f"\nâ±ï¸  å®Œæˆï¼æ€»è€—æ—¶ {elapsed:.2f}s")
        log(f"ğŸ“¤ è¿”å›ç»™å‰ç«¯ï¼šæ™ºèƒ½èåˆåçš„æœ€ç»ˆç­”æ¡ˆ")
        
        return final_output, elapsed

@app.route('/v1/chat/completions', methods=['POST', 'OPTIONS'])
def chat_completions():
    """å…¼å®¹ OpenAI æ ¼å¼çš„èŠå¤©æ¥å£"""
    if request.method == 'OPTIONS':
        return '', 204
    
    try:
        data = request.json
        log(f"ğŸ“¥ æ”¶åˆ°è¯·æ±‚")
        
        messages = data.get('messages', [])
        if not messages:
            return jsonify({"error": {"message": "messages is required"}}), 400
        
        user_message = messages[-1].get('content', '')
        log(f"ğŸ’¬ ç”¨æˆ·æ¶ˆæ¯: {user_message[:50]}...")
        
        # æ‰§è¡Œå¤šé˜¶æ®µæ€è€ƒ
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        result, elapsed = loop.run_until_complete(multi_stage_thinking(user_message))
        loop.close()
        
        # è¿”å› OpenAI æ ¼å¼å“åº”
        response = {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": CONFIG.get('service_model', 'multi-stage-thinking'),
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": result
                },
                "finish_reason": "stop",
                "logprobs": None
            }],
            "usage": {
                "prompt_tokens": len(user_message),
                "completion_tokens": len(result),
                "total_tokens": len(user_message) + len(result)
            }
        }
        
        log(f"ğŸ“¤ è¿”å›å“åº”ï¼Œå†…å®¹é•¿åº¦: {len(result)} å­—ç¬¦")
        return jsonify(response), 200
        
    except Exception as e:
        import traceback
        log(f"âŒ é”™è¯¯: {str(e)}")
        log(f"âŒ è¯¦ç»†: {traceback.format_exc()}")
        return jsonify({"error": {"message": str(e)}}), 500

@app.route('/health', methods=['GET'])
def health():
    return jsonify({"status": "ok"})

if __name__ == '__main__':
    print("\n" + "="*80)
    print("ğŸš€ å¤šé˜¶æ®µæ€è€ƒ API æœåŠ¡å™¨")
    print("="*80)
    print(f"ğŸ“ API åœ°å€: http://127.0.0.1:8000/v1/chat/completions")
    print(f"ğŸ¤– æœåŠ¡æ¨¡å‹: {CONFIG.get('service_model')}")
    print("="*80)
    print("ğŸ“‹ æ€è€ƒæµç¨‹:")
    print("   1. æ¨¡å‹è‡ªå·±æ§åˆ¶ï¼Œæ¯æ­¥400å­—é™åˆ¶")
    print("   2. æ¨¡å‹è‡ªå·±ç”Ÿæˆæ‘˜è¦+æ ‡é¢˜ï¼ˆä¸è®¡å…¥400å­—ï¼‰")
    print("   3. ä¸‹ä¸€æ­¥åªç»™ï¼šç”¨æˆ·æŒ‡ä»¤ + æ‰€æœ‰æ‘˜è¦æ ‡é¢˜ + æ‘˜è¦åº“")
    print("   4. æ¨¡å‹è‡ªå·±é€‰æ‹©éœ€è¦çš„æ‘˜è¦å¹¶æ³¨å…¥")
    print("   5. ç›´åˆ°æ¨¡å‹è¾“å‡ºã€å®Œæˆã€‘")
    print("="*80)
    print("ğŸ”§ ä½¿ç”¨çš„æ¨¡å‹:")
    print(f"   å…¨éƒ¨ä½¿ç”¨: {MODELS['planner']['name']}")
    print("="*80 + "\n")
    
    log("ğŸ¬ æœåŠ¡å™¨å¯åŠ¨ä¸­...")
    app.run(host='0.0.0.0', port=8000, debug=False, threaded=True)
